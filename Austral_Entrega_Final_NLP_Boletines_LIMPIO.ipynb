{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKKizVkbUsKc"
   },
   "source": [
    "# Ingesta de Datos de Boletines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar gdown para descargar desde Google Drive\n",
    "!pip install -q gdown\n",
    "\n",
    "# Descargar el archivo ZIP desde Google Drive (ID del archivo)\n",
    "!gdown --id 1VsKDt8KTn7_n_6slX6vYEaOTkloS9UqP --output boletines.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomprimir el archivo\n",
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = \"boletines.zip\"\n",
    "extract_folder = \"boletines_extraidos\"\n",
    "\n",
    "# Crear carpeta de salida si no existe\n",
    "os.makedirs(extract_folder, exist_ok=True)\n",
    "\n",
    "# Extraer los archivos\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_folder)\n",
    "\n",
    "# Listar archivos extraídos\n",
    "import os\n",
    "archivos = os.listdir(extract_folder)\n",
    "print(\"Archivos extraídos:\")\n",
    "for archivo in archivos:\n",
    "    print(archivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "carpeta = \"/content/boletines_extraidos/Boletines/boletines_2024\" # aqui va la carpeta en drive donde estan los documentos\n",
    "\n",
    "pdfs = [f for f in os.listdir(carpeta) if f.endswith(\".pdf\")]\n",
    "\n",
    "print(\"PDFs encontrados:\", pdfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zKGHEV0UzkN"
   },
   "source": [
    "Por cada pdf o boletin se realiza la lectura y separacion de documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Procesar cada PDF ---\n",
    "df_total = pd.DataFrame()\n",
    "\n",
    "for numero in pdfs:\n",
    "    pdf_path = os.path.join(carpeta, numero)\n",
    "    if not os.path.exists(pdf_path):\n",
    "        continue  # Saltar si no se descargó\n",
    "\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        texto_sumario = \"\"\n",
    "\n",
    "        # Ignorar página 1 y 2\n",
    "        # Ignorar página 0, 1 y la última página\n",
    "        for i in range(2, len(doc) - 1):\n",
    "            texto_sumario += doc[i].get_text()\n",
    "        #for i in range(2, len(doc)):\n",
    "        #    texto_sumario += doc[i].get_text()\n",
    "\n",
    "        # Patrón que detecta bloques finalizados con OP\n",
    "        patron_op = re.compile(r\"OP\\s*N[°º]:\\s*[A-Z]*\\d{6,}\", re.IGNORECASE)\n",
    "        matches = list(patron_op.finditer(texto_sumario))\n",
    "\n",
    "        if not matches:\n",
    "            documentos = [texto_sumario.strip()]\n",
    "        else:\n",
    "            documentos = []\n",
    "            start_idx = 0\n",
    "            for m in matches:\n",
    "                end_idx = m.end()\n",
    "                bloque = texto_sumario[start_idx:end_idx].strip()\n",
    "                documentos.append(bloque)\n",
    "                start_idx = end_idx\n",
    "            if start_idx < len(texto_sumario):\n",
    "                documentos.append(texto_sumario[start_idx:].strip())\n",
    "\n",
    "        # Extraer OP\n",
    "        def extraer_op_final(texto):\n",
    "            match = re.search(r\"OP\\s*N[°º]:\\s*([A-Z]*\\d{6,})\\s*$\", texto.strip(), re.IGNORECASE)\n",
    "            return match.group(1) if match else None\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            \"Boletin_N\": numero,\n",
    "            \"Documento_N\": range(1, len(documentos)+1),\n",
    "            \"Texto\": documentos\n",
    "        })\n",
    "\n",
    "        df[\"OP_Numero\"] = df[\"Texto\"].apply(extraer_op_final)\n",
    "        df_total = pd.concat([df_total, df], ignore_index=True)\n",
    "        print(f\" Procesado boletín {numero} con {len(df)} documentos.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando boletín {numero}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTqNby_nV3dq"
   },
   "source": [
    "# Limpiza de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FS8VVdQVmou"
   },
   "source": [
    "Existen documentos que no estan asociados a un OP_Numero, se trata de encabezados o finales de documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total[df_total[\"OP_Numero\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = df_total[df_total[\"OP_Numero\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3CJBp3jV2Hx"
   },
   "source": [
    "Algunos documentos tienen pie de pagina y encabezados dentro del documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminar_pies_pagina(texto):\n",
    "    lineas = texto.splitlines()\n",
    "    nuevas_lineas = []\n",
    "    skip = 0\n",
    "\n",
    "    for i, linea in enumerate(lineas):\n",
    "        if skip > 0:\n",
    "            skip -= 1\n",
    "            continue\n",
    "        if re.match(r\"Pág\\.\\s*N°\\s*\\d+\", linea.strip()):\n",
    "            skip = 3  # saltar esta línea y las 3 siguientes\n",
    "            continue\n",
    "        nuevas_lineas.append(linea)\n",
    "\n",
    "    return \"\\n\".join(nuevas_lineas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar limpieza\n",
    "df_total[\"Texto_Limpio\"] = df_total[\"Texto\"].apply(eliminar_pies_pagina)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total.loc[1]['Texto']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total.loc[1]['Texto_Limpio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3jw84RKW7hL"
   },
   "source": [
    "# Modelo para predecir etiquetas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                       model=\"hackathon-pln-es/bertin-roberta-base-zeroshot-esnli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dMMCGalGW-3g"
   },
   "source": [
    "## Texto de ejemplo del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(\n",
    "    \"El autor se perfila, a los 50 años de su muerte, como uno de los grandes de su siglo\",\n",
    "    candidate_labels=[\"cultura\", \"sociedad\", \"economia\", \"salud\", \"deportes\"],\n",
    "    hypothesis_template=\"Esta oración es sobre {}.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17MzMSLpXIuS"
   },
   "source": [
    "## Clasificador de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentClassifier:\n",
    "    \"\"\"\n",
    "    Clasificador automático de documentos usando BART-large-MNLI\n",
    "    para clasificación zero-shot de boletines oficiales\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"facebook/bart-large-mnli\"):\n",
    "        \"\"\"\n",
    "        Inicializa el clasificador\n",
    "\n",
    "        Args:\n",
    "            model_name: Nombre del modelo de Hugging Face\n",
    "        \"\"\"\n",
    "        print(\"Cargando modelo BART-large-MNLI...\")\n",
    "        self.classifier = pipeline(\n",
    "            \"zero-shot-classification\",\n",
    "            model=model_name,\n",
    "            device=0 if torch.cuda.is_available() else -1  # GPU si está disponible\n",
    "        )\n",
    "        print(\"Modelo cargado exitosamente!\")\n",
    "\n",
    "        # Etiquetas para clasificación\n",
    "        self.etiquetas_boletin = [\n",
    "            \"Leyes\",\n",
    "            \"Decisiones Administrativas\",\n",
    "            \"Resoluciones Delegadas\",\n",
    "            \"Resoluciones Ministeriales\",\n",
    "            \"Resoluciones (Secretaría de Obras Públicas)\",\n",
    "            \"Licitaciones Públicas\",\n",
    "            \"Adjudicaciones Simples\",\n",
    "            \"Contrataciones Abreviadas\",\n",
    "            \"Concesiones de Agua Pública\",\n",
    "            \"Sentencias\",\n",
    "            \"Sucesorios\",\n",
    "            \"Edictos de Quiebras\",\n",
    "            \"Concursos Civiles o Preventivos\",\n",
    "            \"Edictos Judiciales\",\n",
    "            \"Constituciones de Sociedad\",\n",
    "            \"Asambleas Comerciales\",\n",
    "            \"Asambleas Civiles\",\n",
    "            \"Avisos Generales\",\n",
    "            \"Recaudación\"\n",
    "        ]\n",
    "\n",
    "    def preprocess_text(self, text: str, max_length: int = 512) -> str:\n",
    "        \"\"\"\n",
    "        Preprocesa el texto para optimizar la clasificación\n",
    "\n",
    "        Args:\n",
    "            text: Texto a procesar\n",
    "            max_length: Longitud máxima del texto\n",
    "\n",
    "        Returns:\n",
    "            Texto preprocesado\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return \"\"\n",
    "\n",
    "        # Limpiar texto básico\n",
    "        text = text.strip()\n",
    "\n",
    "        # Tomar principalmente el inicio del documento (más informativo)\n",
    "        # y algo del final si es muy largo\n",
    "        if len(text) > max_length:\n",
    "            # Tomar primeros 400 caracteres y últimos 100\n",
    "            text = text[:400] + \"...\" + text[-100:]\n",
    "\n",
    "        return text\n",
    "\n",
    "    def classify_single_document(self, text: str, threshold: float = 0.5) -> Dict:\n",
    "        \"\"\"\n",
    "        Clasifica un solo documento\n",
    "\n",
    "        Args:\n",
    "            text: Texto del documento\n",
    "            threshold: Umbral mínimo de confianza\n",
    "\n",
    "        Returns:\n",
    "            Diccionario con resultado de clasificación\n",
    "        \"\"\"\n",
    "        # Preprocesar texto\n",
    "        processed_text = self.preprocess_text(text)\n",
    "\n",
    "        if not processed_text:\n",
    "            return {\n",
    "                'etiqueta_predicha': 'Sin clasificar',\n",
    "                'confianza': 0.0,\n",
    "                'top_3_etiquetas': [],\n",
    "                'scores_completos': {}\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            # Realizar clasificación\n",
    "            resultado = self.classifier(\n",
    "                processed_text,\n",
    "                self.etiquetas_boletin,\n",
    "                multi_label=False\n",
    "            )\n",
    "\n",
    "            # Extraer resultados\n",
    "            etiqueta_principal = resultado['labels'][0]\n",
    "            confianza_principal = resultado['scores'][0]\n",
    "\n",
    "            # Top 3 etiquetas con scores\n",
    "            top_3 = [\n",
    "                {\n",
    "                    'etiqueta': resultado['labels'][i],\n",
    "                    'score': resultado['scores'][i]\n",
    "                }\n",
    "                for i in range(min(3, len(resultado['labels'])))\n",
    "            ]\n",
    "\n",
    "            # Scores completos\n",
    "            scores_completos = dict(zip(resultado['labels'], resultado['scores']))\n",
    "\n",
    "            # Aplicar umbral de confianza\n",
    "            if confianza_principal < threshold:\n",
    "                etiqueta_final = 'Clasificación incierta'\n",
    "            else:\n",
    "                etiqueta_final = etiqueta_principal\n",
    "\n",
    "            return {\n",
    "                'etiqueta_predicha': etiqueta_final,\n",
    "                'confianza': confianza_principal,\n",
    "                'top_3_etiquetas': top_3,\n",
    "                'scores_completos': scores_completos\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error en clasificación: {str(e)}\")\n",
    "            return {\n",
    "                'etiqueta_predicha': 'Error en clasificación',\n",
    "                'confianza': 0.0,\n",
    "                'top_3_etiquetas': [],\n",
    "                'scores_completos': {}\n",
    "            }\n",
    "\n",
    "    def classify_dataframe(self, df: pd.DataFrame, text_column: str = 'Texto',\n",
    "                          threshold: float = 0.5, batch_size: int = 10) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Clasifica todos los documentos en un DataFrame\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame con los documentos\n",
    "            text_column: Nombre de la columna con el texto\n",
    "            threshold: Umbral de confianza\n",
    "            batch_size: Tamaño de lote para procesamiento\n",
    "\n",
    "        Returns:\n",
    "            DataFrame con las clasificaciones agregadas\n",
    "        \"\"\"\n",
    "        print(f\"Clasificando {len(df)} documentos...\")\n",
    "\n",
    "        # Copiar DataFrame para no modificar el original\n",
    "        df_resultado = df.copy()\n",
    "\n",
    "        # Listas para almacenar resultados\n",
    "        etiquetas_predichas = []\n",
    "        confianzas = []\n",
    "        top_3_lists = []\n",
    "\n",
    "        # Procesar en lotes para mostrar progreso\n",
    "        for i in range(0, len(df), batch_size):\n",
    "            batch_end = min(i + batch_size, len(df))\n",
    "            print(f\"Procesando documentos {i+1}-{batch_end} de {len(df)}\")\n",
    "\n",
    "            # Procesar cada documento en el lote\n",
    "            for idx in range(i, batch_end):\n",
    "                texto = df.iloc[idx][text_column]\n",
    "                resultado = self.classify_single_document(texto, threshold)\n",
    "\n",
    "                etiquetas_predichas.append(resultado['etiqueta_predicha'])\n",
    "                confianzas.append(resultado['confianza'])\n",
    "                top_3_lists.append(resultado['top_3_etiquetas'])\n",
    "\n",
    "        # Agregar resultados al DataFrame\n",
    "        df_resultado['Etiqueta_Predicha'] = etiquetas_predichas\n",
    "        df_resultado['Confianza'] = confianzas\n",
    "        df_resultado['Top_3_Etiquetas'] = top_3_lists\n",
    "\n",
    "        print(\"Clasificación completada!\")\n",
    "        return df_resultado\n",
    "\n",
    "    def get_classification_summary(self, df_classified: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Genera un resumen de las clasificaciones\n",
    "\n",
    "        Args:\n",
    "            df_classified: DataFrame con clasificaciones\n",
    "\n",
    "        Returns:\n",
    "            DataFrame con resumen estadístico\n",
    "        \"\"\"\n",
    "        summary = df_classified.groupby('Etiqueta_Predicha').agg({\n",
    "            'Confianza': ['count', 'mean', 'std', 'min', 'max']\n",
    "        }).round(3)\n",
    "\n",
    "        summary.columns = ['Cantidad', 'Confianza_Media', 'Confianza_Std', 'Confianza_Min', 'Confianza_Max']\n",
    "        summary = summary.reset_index()\n",
    "        summary = summary.sort_values('Cantidad', ascending=False)\n",
    "\n",
    "        return summary\n",
    "\n",
    "    def analyze_low_confidence_predictions(self, df_classified: pd.DataFrame,\n",
    "                                         threshold: float = 0.7) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Analiza las predicciones con baja confianza para revisión manual\n",
    "\n",
    "        Args:\n",
    "            df_classified: DataFrame con clasificaciones\n",
    "            threshold: Umbral para considerar baja confianza\n",
    "\n",
    "        Returns:\n",
    "            DataFrame con documentos de baja confianza\n",
    "        \"\"\"\n",
    "        low_confidence = df_classified[df_classified['Confianza'] < threshold].copy()\n",
    "\n",
    "        if len(low_confidence) > 0:\n",
    "            print(f\"Encontrados {len(low_confidence)} documentos con confianza < {threshold}\")\n",
    "            print(\"Estos documentos podrían requerir revisión manual.\")\n",
    "\n",
    "        return low_confidence.sort_values('Confianza')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBHa-y6IXQIe"
   },
   "source": [
    "Instanciamos el clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificador = DocumentClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado_individual = clasificador.classify_single_document(df_total.iloc[0]['Texto_Limpio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Documento: {df_total.iloc[0]['Texto_Limpio']}...\")\n",
    "print(f\"  Etiqueta predicha: {resultado_individual['etiqueta_predicha']}\")\n",
    "print(f\" Confianza: {resultado_individual['confianza']:.3f}\")\n",
    "print(\" Top 3 etiquetas:\")\n",
    "for i, item in enumerate(resultado_individual['top_3_etiquetas'][:3]):\n",
    "    print(f\"   {i+1}. {item['etiqueta']}: {item['score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rVuQPBmY-ac"
   },
   "source": [
    "## Clasificador Masivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentClassifierOptimized:\n",
    "    \"\"\"\n",
    "    Versión optimizada del clasificador para procesar grandes volúmenes de documentos\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"facebook/bart-large-mnli\"):\n",
    "        \"\"\"\n",
    "        Inicializa el clasificador optimizado\n",
    "        \"\"\"\n",
    "        print(\"Cargando modelo BART-large-MNLI...\")\n",
    "\n",
    "        # Configuración optimizada del pipeline\n",
    "        self.classifier = pipeline(\n",
    "            \"zero-shot-classification\",\n",
    "            model=model_name,\n",
    "            device=0 if torch.cuda.is_available() else -1,\n",
    "            # Optimizaciones de memoria\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            model_kwargs={\n",
    "                \"low_cpu_mem_usage\": True,\n",
    "                \"use_cache\": False  # Reduce memoria\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(f\"Modelo cargado en: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "        # Etiquetas para clasificación\n",
    "        self.etiquetas_boletin = [\n",
    "            \"Leyes\",\n",
    "            \"Decisiones Administrativas\",\n",
    "            \"Resoluciones Delegadas\",\n",
    "            \"Resoluciones Ministeriales\",\n",
    "            \"Resoluciones (Secretaría de Obras Públicas)\",\n",
    "            \"Licitaciones Públicas\",\n",
    "            \"Adjudicaciones Simples\",\n",
    "            \"Contrataciones Abreviadas\",\n",
    "            \"Concesiones de Agua Pública\",\n",
    "            \"Sentencias\",\n",
    "            \"Sucesorios\",\n",
    "            \"Edictos de Quiebras\",\n",
    "            \"Concursos Civiles o Preventivos\",\n",
    "            \"Edictos Judiciales\",\n",
    "            \"Constituciones de Sociedad\",\n",
    "            \"Asambleas Comerciales\",\n",
    "            \"Asambleas Civiles\",\n",
    "            \"Avisos Generales\",\n",
    "            \"Recaudación\"\n",
    "        ]\n",
    "\n",
    "    def preprocess_batch_texts(self, texts: List[str], max_length: int = 400) -> List[str]:\n",
    "        \"\"\"\n",
    "        Preprocesa un lote de textos de manera eficiente\n",
    "        \"\"\"\n",
    "        processed_texts = []\n",
    "\n",
    "        for text in texts:\n",
    "            if not isinstance(text, str) or not text.strip():\n",
    "                processed_texts.append(\"\")\n",
    "                continue\n",
    "\n",
    "            text = text.strip()\n",
    "\n",
    "            # Truncar texto de manera inteligente\n",
    "            if len(text) > max_length:\n",
    "                # Tomar inicio y final del texto\n",
    "                text = text[:int(max_length*0.8)] + \"...\" + text[-int(max_length*0.2):]\n",
    "\n",
    "            processed_texts.append(text)\n",
    "\n",
    "        return processed_texts\n",
    "\n",
    "    def classify_batch(self, texts: List[str]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Clasifica un lote de textos de manera eficiente\n",
    "        \"\"\"\n",
    "        # Preprocesar lote\n",
    "        processed_texts = self.preprocess_batch_texts(texts)\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for text in processed_texts:\n",
    "            if not text:\n",
    "                # Resultado vacío para textos sin contenido\n",
    "                empty_result = {etiqueta: 0.0 for etiqueta in self.etiquetas_boletin}\n",
    "                results.append(empty_result)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Clasificar texto individual\n",
    "                resultado = self.classifier(\n",
    "                    text,\n",
    "                    self.etiquetas_boletin,\n",
    "                    multi_label=False\n",
    "                )\n",
    "\n",
    "                # Convertir a diccionario de scores\n",
    "                scores_dict = dict(zip(resultado['labels'], resultado['scores']))\n",
    "\n",
    "                # Asegurar que todas las etiquetas estén presentes\n",
    "                complete_scores = {}\n",
    "                for etiqueta in self.etiquetas_boletin:\n",
    "                    complete_scores[etiqueta] = scores_dict.get(etiqueta, 0.0)\n",
    "\n",
    "                results.append(complete_scores)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error procesando texto: {str(e)[:100]}...\")\n",
    "                # Resultado con scores en 0 en caso de error\n",
    "                error_result = {etiqueta: 0.0 for etiqueta in self.etiquetas_boletin}\n",
    "                results.append(error_result)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def classify_dataframe_optimized(self,\n",
    "                                   df: pd.DataFrame,\n",
    "                                   text_column: str,\n",
    "                                   batch_size: int = 8,\n",
    "                                   save_progress: bool = True,\n",
    "                                   checkpoint_every: int = 1000) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Clasifica DataFrame completo con optimizaciones para grandes volúmenes\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame con documentos\n",
    "            text_column: Nombre de columna con texto\n",
    "            batch_size: Tamaño de lote (reducido para optimizar memoria)\n",
    "            save_progress: Si guardar progreso periódicamente\n",
    "            checkpoint_every: Cada cuántos documentos guardar checkpoint\n",
    "\n",
    "        Returns:\n",
    "            DataFrame con columnas de scores para cada etiqueta\n",
    "        \"\"\"\n",
    "        print(f\"Iniciando clasificación de {len(df)} documentos...\")\n",
    "        print(f\"Batch size: {batch_size}\")\n",
    "        print(f\"Etiquetas a clasificar: {len(self.etiquetas_boletin)}\")\n",
    "\n",
    "        # Copiar DataFrame\n",
    "        df_resultado = df.copy()\n",
    "\n",
    "        # Inicializar columnas de scores\n",
    "        for etiqueta in self.etiquetas_boletin:\n",
    "            df_resultado[f'Score_{etiqueta}'] = 0.0\n",
    "\n",
    "        # Variables para tracking\n",
    "        total_batches = (len(df) + batch_size - 1) // batch_size\n",
    "        start_time = time.time()\n",
    "        processed_docs = 0\n",
    "\n",
    "        # Barra de progreso\n",
    "        pbar = tqdm(total=len(df), desc=\"Clasificando documentos\")\n",
    "\n",
    "        try:\n",
    "            # Procesar en lotes\n",
    "            for batch_idx in range(0, len(df), batch_size):\n",
    "                batch_end = min(batch_idx + batch_size, len(df))\n",
    "\n",
    "                # Extraer textos del lote\n",
    "                batch_texts = df.iloc[batch_idx:batch_end][text_column].tolist()\n",
    "\n",
    "                # Clasificar lote\n",
    "                batch_results = self.classify_batch(batch_texts)\n",
    "\n",
    "                # Asignar resultados al DataFrame\n",
    "                for i, scores_dict in enumerate(batch_results):\n",
    "                    doc_idx = batch_idx + i\n",
    "                    for etiqueta, score in scores_dict.items():\n",
    "                        df_resultado.loc[doc_idx, f'Score_{etiqueta}'] = score\n",
    "\n",
    "                # Actualizar progreso\n",
    "                processed_docs += len(batch_texts)\n",
    "                pbar.update(len(batch_texts))\n",
    "\n",
    "                # Estadísticas de tiempo\n",
    "                elapsed_time = time.time() - start_time\n",
    "                docs_per_second = processed_docs / elapsed_time\n",
    "                remaining_docs = len(df) - processed_docs\n",
    "                eta_seconds = remaining_docs / docs_per_second if docs_per_second > 0 else 0\n",
    "\n",
    "                pbar.set_postfix({\n",
    "                    'Docs/s': f'{docs_per_second:.2f}',\n",
    "                    'ETA': f'{eta_seconds/60:.1f}min'\n",
    "                })\n",
    "\n",
    "                # Checkpoint periódico\n",
    "                if save_progress and processed_docs % checkpoint_every == 0:\n",
    "                    checkpoint_file = f'classification_checkpoint_{processed_docs}.pkl'\n",
    "                    df_resultado.to_pickle(checkpoint_file)\n",
    "                    # Ahora lo descargás automáticamente\n",
    "\n",
    "                    print(f\"\\nCheckpoint guardado: {checkpoint_file}\")\n",
    "\n",
    "                # Limpiar memoria periódicamente\n",
    "                if batch_idx % (batch_size * 10) == 0:\n",
    "                    gc.collect()\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "\n",
    "        finally:\n",
    "            pbar.close()\n",
    "\n",
    "        # Agregar columnas de análisis\n",
    "        df_resultado = self._add_analysis_columns(df_resultado)\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\n✅ Clasificación completada!\")\n",
    "        print(f\"⏱️  Tiempo total: {total_time/60:.2f} minutos\")\n",
    "        print(f\"📊 Velocidad promedio: {len(df)/total_time:.2f} docs/segundo\")\n",
    "\n",
    "        return df_resultado\n",
    "\n",
    "    def _add_analysis_columns(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Agrega columnas de análisis basadas en los scores\n",
    "        \"\"\"\n",
    "        print(\"Agregando columnas de análisis...\")\n",
    "\n",
    "        # Columnas de scores\n",
    "        score_columns = [col for col in df.columns if col.startswith('Score_')]\n",
    "\n",
    "        # Etiqueta con mayor score\n",
    "        df['Etiqueta_Predicha'] = df[score_columns].idxmax(axis=1).str.replace('Score_', '')\n",
    "\n",
    "        # Confianza máxima\n",
    "        df['Confianza_Maxima'] = df[score_columns].max(axis=1)\n",
    "\n",
    "        # Top 3 etiquetas\n",
    "        def get_top_3(row):\n",
    "            scores = [(col.replace('Score_', ''), row[col]) for col in score_columns]\n",
    "            scores.sort(key=lambda x: x[1], reverse=True)\n",
    "            return scores[:3]\n",
    "\n",
    "        df['Top_3_Etiquetas'] = df.apply(get_top_3, axis=1)\n",
    "\n",
    "        # Diferencia entre top 2 (indica certeza)\n",
    "        def get_confidence_gap(row):\n",
    "            scores = [row[col] for col in score_columns]\n",
    "            scores.sort(reverse=True)\n",
    "            return scores[0] - scores[1] if len(scores) > 1 else 0\n",
    "\n",
    "        df['Gap_Confianza'] = df.apply(get_confidence_gap, axis=1)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def get_classification_summary(self, df_classified: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Genera resumen estadístico de las clasificaciones\n",
    "        \"\"\"\n",
    "        print(\"Generando resumen de clasificación...\")\n",
    "\n",
    "        summary = df_classified.groupby('Etiqueta_Predicha').agg({\n",
    "            'Confianza_Maxima': ['count', 'mean', 'std', 'min', 'max'],\n",
    "            'Gap_Confianza': 'mean'\n",
    "        }).round(4)\n",
    "\n",
    "        summary.columns = ['Cantidad', 'Confianza_Media', 'Confianza_Std',\n",
    "                          'Confianza_Min', 'Confianza_Max', 'Gap_Promedio']\n",
    "        summary = summary.reset_index()\n",
    "        summary = summary.sort_values('Cantidad', ascending=False)\n",
    "\n",
    "        # Calcular porcentajes\n",
    "        summary['Porcentaje'] = (summary['Cantidad'] / len(df_classified) * 100).round(2)\n",
    "\n",
    "        return summary\n",
    "\n",
    "    def save_results(self, df_classified: pd.DataFrame, filename: str = \"documentos_clasificados\"):\n",
    "        \"\"\"\n",
    "        Guarda los resultados en múltiples formatos\n",
    "        \"\"\"\n",
    "        print(f\"Guardando resultados como {filename}...\")\n",
    "\n",
    "        # Guardar DataFrame completo\n",
    "        df_classified.to_pickle(f\"{filename}.pkl\")\n",
    "        df_classified.to_csv(f\"{filename}.csv\", index=False)\n",
    "\n",
    "        # Guardar solo scores en archivo separado\n",
    "        score_columns = [col for col in df_classified.columns if col.startswith('Score_')]\n",
    "        analysis_columns = ['Etiqueta_Predicha', 'Confianza_Maxima', 'Gap_Confianza']\n",
    "\n",
    "        df_scores = df_classified[score_columns + analysis_columns]\n",
    "        df_scores.to_csv(f\"{filename}_scores_only.csv\", index=False)\n",
    "\n",
    "        print(\"✅ Resultados guardados!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opción 2: Usando la clase (más control)\n",
    "classifier = DocumentClassifierOptimized()\n",
    "df_classified = classifier.classify_dataframe_optimized(\n",
    "    df=df_total,\n",
    "    text_column=\"Texto_Limpio\",\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supongamos que termina tu análisis y guardás el archivo\n",
    "df_classified.to_csv(\"resumen.csv\", index=False)\n",
    "\n",
    "# Ahora lo descargás automáticamente\n",
    "files.download(\"resumen.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dWC8Sh8rUPq"
   },
   "source": [
    "# Prediccion en la Busqueda de Documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sentence-transformers faiss-cpu pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Importar librerías\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "\n",
    "\n",
    "file_path = \"/content/resumen.csv\"\n",
    "# Aumentar el límite de campo para campos muy largos\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "# Forzar lectura tolerante a errores y texto largo\n",
    "df = pd.read_csv(\n",
    "    file_path,\n",
    "    encoding=\"utf-8\",\n",
    "    quoting=csv.QUOTE_ALL,  # requiere que todos los campos estén entre comillas\n",
    "    engine=\"python\",\n",
    "    on_bad_lines=\"warn\"  # mostrar advertencias si encuentra líneas mal formadas\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplazá por la ruta correcta de tu archivo\n",
    "#df = pd.read_pickle(\"/content/classification_checkpoint_15000.pkl\")\n",
    "\n",
    "# Verificá los primeros registros\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"Texto_Limpio\"])  # aseguramos que no haya nulos\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Opcional: Previsualización\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAosAdlMvMb5"
   },
   "source": [
    "## Evaluacion de la Clasificacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IR3AP8FivO1V"
   },
   "source": [
    "Evaluación de la Clasificación Automática – Validación por Muestreo\n",
    "Para validar el rendimiento del modelo de clasificación automática sin etiquetar manualmente los 15.000 documentos, aplicamos estadística inferencial para determinar un tamaño de muestra representativo.\n",
    "\n",
    "Parámetros definidos:\n",
    "Tamaño de población (N): 15.000 documentos\n",
    "\n",
    "Nivel de confianza: 90%\n",
    "\n",
    "Margen de error permitido: ±10%\n",
    "\n",
    "Proporción esperada (p): 0.5 (caso conservador)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Parámetros\n",
    "N = 15114           # Población\n",
    "Z = 1.645           # Z-score para 90% confianza\n",
    "e = 0.10            # Margen de error\n",
    "p = 0.5             # Proporción esperada\n",
    "\n",
    "# Tamaño de muestra sin corrección\n",
    "n_0 = (Z**2 * p * (1 - p)) / (e**2)\n",
    "\n",
    "# Corrección poblacional finita\n",
    "n = n_0 / (1 + (n_0 - 1) / N)\n",
    "n = int(np.ceil(n))\n",
    "\n",
    "print(f\"📏 Tamaño óptimo de muestra: {n} documentos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_muestra = df.sample(n=n, random_state=42).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_muestra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_muestra.to_excel(\"muestra_validacion_modelo.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruir el dataset entregado por el usuario\n",
    "data = {\n",
    "    \"Etiqueta_Predicha\": [\n",
    "        \"Resoluciones Delegadas\", \"Adjudicaciones Simples\", \"Contrataciones Abreviadas\", \"Licitaciones Públicas\",\n",
    "        \"Contrataciones Abreviadas\", \"Sucesorios\", \"Asambleas Comerciales\", \"Edictos de Quiebras\", \"Contrataciones Abreviadas\",\n",
    "        \"Resoluciones Ministeriales\", \"Sucesorios\", \"Constituciones de Sociedad\", \"Concursos Civiles o Preventivos\",\n",
    "        \"Sucesorios\", \"Contrataciones Abreviadas\", \"Resoluciones (Secretaría de Obras Públicas)\", \"Decisiones Administrativas\",\n",
    "        \"Resoluciones Ministeriales\", \"Decisiones Administrativas\", \"Sucesorios\", \"Asambleas Comerciales\", \"Licitaciones Públicas\",\n",
    "        \"Sucesorios\", \"Sucesorios\", \"Resoluciones Ministeriales\", \"Sentencias\", \"Contrataciones Abreviadas\",\n",
    "        \"Concesiones de Agua Pública\", \"Resoluciones Ministeriales\", \"Edictos de Quiebras\", \"Decisiones Administrativas\",\n",
    "        \"Decisiones Administrativas\", \"Decisiones Administrativas\", \"Sucesorios\", \"Sucesorios\", \"Sucesorios\",\n",
    "        \"Resoluciones Ministeriales\", \"Resoluciones Delegadas\", \"Leyes\", \"Contrataciones Abreviadas\", \"Sucesorios\",\n",
    "        \"Resoluciones Ministeriales\", \"Contrataciones Abreviadas\", \"Adjudicaciones Simples\", \"Sentencias\",\n",
    "        \"Resoluciones Ministeriales\", \"Sucesorios\", \"Recaudación\", \"Leyes\", \"Adjudicaciones Simples\",\n",
    "        \"Contrataciones Abreviadas\", \"Licitaciones Públicas\", \"Resoluciones Ministeriales\", \"Asambleas Comerciales\",\n",
    "        \"Contrataciones Abreviadas\", \"Sucesorios\", \"Sucesorios\", \"Resoluciones Delegadas\", \"Contrataciones Abreviadas\",\n",
    "        \"Sucesorios\", \"Edictos de Quiebras\", \"Sentencias\", \"Adjudicaciones Simples\", \"Contrataciones Abreviadas\",\n",
    "        \"Licitaciones Públicas\", \"Decisiones Administrativas\", \"Licitaciones Públicas\"\n",
    "    ],\n",
    "    \"Etiqueta_Verdadera\": [\n",
    "        \"Avisos Generales\", \"Adjudicaciones Simples\", \"Contrataciones Abreviadas\", \"Licitaciones Públicas\",\n",
    "        \"Contrataciones Abreviadas\", \"Sucesorios\", \"Asambleas Comerciales\", \"Edictos de Quiebras\", \"POSESIONES VEINTEAÑALES\",\n",
    "        \"Decretos\", \"Sucesorios\", \"Constituciones de Sociedad\", \"REMATES JUDICIALES\", \"Sucesorios\", \"Contrataciones Abreviadas\",\n",
    "        \"Resoluciones (Secretaría de Obras Públicas)\", \"Decisiones Administrativas\", \"ASAMBLEAS CIVILES\",\n",
    "        \"Decisiones Administrativas\", \"Sucesorios\", \"Avisos Comerciales\", \"CONVOCATORIAS A AUDIENCIA PÚBLICA\",\n",
    "        \"Sucesorios\", \"Sucesorios\", \"Decretos\", \"POSESIONES VEINTEAÑALES\", \"Contrataciones Abreviadas\",\n",
    "        \"Concesiones de Agua Pública\", \"Resoluciones Ministeriales\", \"Edictos de Quiebras\", \"NOTIFICACIONES ADMINISTRATIVAS\",\n",
    "        \"Decisiones Administrativas\", \"Decisiones Administrativas\", \"Sucesorios\", \"Sucesorios\", \"Sucesorios\",\n",
    "        \"Resoluciones Delegadas\", \"Resoluciones Delegadas\", \"NOTIFICACIONES ADMINISTRATIVAS\", \"Contrataciones Abreviadas\",\n",
    "        \"Sucesorios\", \"Resoluciones Delegadas\", \"Contrataciones Abreviadas\", \"Adjudicaciones Simples\", \"EDICTOS DE MINAS\",\n",
    "        \"DECRETOS\", \"Sucesorios\", \"NOTIFICACIONES ADMINISTRATIVAS\", \"EDICTOS DE MINAS\", \"Adjudicaciones Simples\",\n",
    "        \"Contrataciones Abreviadas\", \"Licitaciones Públicas\", \"Resoluciones Ministeriales\", \"Asambleas Comerciales\",\n",
    "        \"Contrataciones Abreviadas\", \"Sucesorios\", \"Sucesorios\", \"Resoluciones Delegadas\", \"Contrataciones Abreviadas\",\n",
    "        \"Sucesorios\", \"Edictos de Quiebras\", \"NOTIFICACIONES ADMINISTRATIVAS\", \"Adjudicaciones Simples\",\n",
    "        \"Contrataciones Abreviadas\", \"Licitaciones Públicas\", \"Decisiones Administrativas\", \"Licitaciones Públicas\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataFrame\n",
    "df_eval = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar etiquetas verdaderas no vistas por el modelo\n",
    "etiquetas_predichas = set(df_eval[\"Etiqueta_Predicha\"])\n",
    "etiquetas_verdaderas = set(df_eval[\"Etiqueta_Verdadera\"])\n",
    "etiquetas_desconocidas = etiquetas_verdaderas - etiquetas_predichas\n",
    "\n",
    "# Filtrar para el reporte solo etiquetas evaluables\n",
    "df_eval_filtrado = df_eval[~df_eval[\"Etiqueta_Verdadera\"].isin(etiquetas_desconocidas)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear reporte\n",
    "reporte = classification_report(\n",
    "    df_eval_filtrado[\"Etiqueta_Verdadera\"],\n",
    "    df_eval_filtrado[\"Etiqueta_Predicha\"],\n",
    "    output_dict=True,\n",
    "    zero_division=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reporte = pd.DataFrame(reporte).transpose()\n",
    "df_reporte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etiquetas_desconocidas_sorted = sorted(etiquetas_desconocidas)\n",
    "etiquetas_desconocidas_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la matriz de confusión\n",
    "y_true = df_eval_filtrado[\"Etiqueta_Verdadera\"]\n",
    "y_pred = df_eval_filtrado[\"Etiqueta_Predicha\"]\n",
    "labels = sorted(y_true.unique())\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "# Crear el heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel(\"Etiqueta Predicha\")\n",
    "plt.ylabel(\"Etiqueta Verdadera\")\n",
    "plt.title(\"Matriz de Confusión\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calcular accuracy general\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v99ZRfVmvnzv"
   },
   "source": [
    "## Prediccion de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear embeddings con un modelo de Hugging Face\n",
    "# Usamos un modelo multilingüe adecuado para textos legales\n",
    "modelo = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Convertimos cada contenido a un vector (embedding)\n",
    "corpus = df['Texto_Limpio'].tolist()\n",
    "embeddings = modelo.encode(corpus, show_progress_bar=True)\n",
    "\n",
    "# Convertimos a matriz numpy\n",
    "embedding_matrix = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 4: Indexar embeddings con FAISS (búsqueda rápida por similitud)\n",
    "dim = embedding_matrix.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buscar_respuesta(pregunta, top_k=5, mostrar=True):\n",
    "    \"\"\"\n",
    "    Busca los documentos más relevantes para una pregunta usando embeddings semánticos.\n",
    "\n",
    "    Args:\n",
    "        pregunta (str): pregunta del usuario\n",
    "        top_k (int): cantidad de documentos a mostrar\n",
    "        mostrar (bool): si se desea imprimir resultados o no\n",
    "\n",
    "    Returns:\n",
    "        df_resultado (pd.DataFrame): sub-DataFrame con los documentos más similares\n",
    "        lista_indices (List[int]): lista de índices del DataFrame original\n",
    "    \"\"\"\n",
    "    pregunta_emb = modelo.encode([pregunta])\n",
    "    D, I = index.search(np.array(pregunta_emb), top_k)\n",
    "\n",
    "    indices = I[0].tolist()\n",
    "    distancias = D[0]\n",
    "    resultados = df.iloc[indices].copy()\n",
    "    resultados[\"Distancia\"] = distancias\n",
    "\n",
    "    if mostrar:\n",
    "        for i, (idx, dist) in enumerate(zip(indices, distancias)):\n",
    "            print(f\"\\n🔎 Documento {i+1} (Distancia {dist:.2f})\")\n",
    "            print(f\"OP_Numero: {df.iloc[idx]['OP_Numero']}\")\n",
    "            print(f\"Contenido:\\n{df.iloc[idx]['Texto_Limpio'][:2000]}...\")\n",
    "\n",
    "    return resultados, indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 6: Probar el sistema\n",
    "pregunta = \"¿Qué documentos mencionan adjudicaciones?\"\n",
    "resultados = buscar_respuesta(pregunta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['OP_Numero']==\"100114953\"]['Texto_Limpio'].values[0]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
